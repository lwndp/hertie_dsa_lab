{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 9: A Recurrence Relation \\& The Master Theorem\n",
    "## Data Structures & Algorithms\n",
    "10&11/04/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember MergeSort? \n",
    "\n",
    "## A Divide-&-Conquer refresher\n",
    "Divide-and-conquer algorithms are a class of algorithms that solve a problem by:\n",
    "1. **Divide**: Breaking the problem into smaller, more manageable subproblems.\n",
    "2. **Conquer**: Solving the subproblems recursively.\n",
    "3. **Combine**: Merging the solutions of the subproblems to solve the original problem.\n",
    "\n",
    "## MergeSort\n",
    "1. divides the array into two halves, \n",
    "2. sorts each half recursively, \n",
    "3. and then merges the sorted halves.\n",
    "\n",
    "<div>\n",
    "   <img src=\"images/mergesort_viz.png\" width=\"500px\" title=\"mergesort visualisation\">\n",
    "</div>\n",
    "\n",
    "## In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    \"\"\"\n",
    "    Merge sort \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : a list of number\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    The list sorted in ascending order\n",
    "    \"\"\"\n",
    "    arr_temp = list(arr)\n",
    "    n = len(arr_temp)    \n",
    "    \n",
    "    if n > 1: \n",
    "        # STEP 1: DIVIDE\n",
    "        # Divide the list into two smaller ones\n",
    "        # The middle of the list\n",
    "        mid = n // 2 # using floor division (a.k.a integer division)\n",
    "        # The left sublist\n",
    "        arr_temp_left = arr_temp[:mid] \n",
    "        # The right sublist\n",
    "        arr_temp_right = arr_temp[mid:]\n",
    "\n",
    "        # STEP 2: RECURSIVE CALL (UNTIL N=1)\n",
    "        # Recursively call merge_sort to sort the two smaller lists\n",
    "        arr_temp_left = merge_sort(arr_temp_left)\n",
    "        arr_temp_right = merge_sort(arr_temp_right)\n",
    "        \n",
    "        # STEP 3: MERGE  \n",
    "        # Merge the two sorted smaller lists\n",
    "        i = j = k = 0\n",
    "        n_left, n_right = len(arr_temp_left), len(arr_temp_right)\n",
    "          \n",
    "        while i < n_left and j < n_right: # this while statement says to keep going through the two lists until one of them is exhausted\n",
    "            if arr_temp_left[i] < arr_temp_right[j]: \n",
    "                arr_temp[k] = arr_temp_left[i] \n",
    "                i += 1\n",
    "            else: \n",
    "                arr_temp[k] = arr_temp_right[j] \n",
    "                j += 1\n",
    "            k += 1\n",
    "          \n",
    "        # If there are elements in arr_temp_left that have not been visited \n",
    "        while i < n_left: \n",
    "            arr_temp[k] = arr_temp_left[i] \n",
    "            i += 1\n",
    "            k += 1\n",
    " \n",
    "        # If there are elements in arr_temp_right that have not been visited \n",
    "        while j < n_right: \n",
    "            arr_temp[k] = arr_temp_right[j] \n",
    "            j += 1\n",
    "            k += 1\n",
    "            \n",
    "    return arr_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compared to brute force BubbleSort\n",
    "\n",
    "Remember, BubbleSort (naive / brute force) was $O(n^2)$; whereas MergeSort (efficient) was $O(n\\log(n))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bubble Sort Time: 0.9977 seconds\n",
      "Merge Sort Time:  0.0068 seconds\n",
      "Merge sort is 145.71 times faster than bubble sort\n"
     ]
    }
   ],
   "source": [
    "def bubble_sort(arr):\n",
    "    \"\"\"\n",
    "    Bubble sort\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : a list of number\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    The list sorted in ascending order\n",
    "    \"\"\"\n",
    "    n = len(arr)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(0, n-i-1):\n",
    "            if arr[j] > arr[j+1]:\n",
    "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
    "                \n",
    "    return arr\n",
    "\n",
    "# execute:\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "random.seed(42)\n",
    "long_arr = [random.randint(0, 10_000) for _ in range(5000)]\n",
    "\n",
    "# let's time bubble sort\n",
    "start_bubble = time.time()\n",
    "bubble_sort(long_arr)\n",
    "end_bubble = time.time()\n",
    "bubble_time = end_bubble - start_bubble\n",
    "\n",
    "# ...and merge sort\n",
    "start_merge = time.time()\n",
    "merge_sort(long_arr)\n",
    "end_merge = time.time()\n",
    "merge_time = end_merge - start_merge\n",
    "\n",
    "# Print results\n",
    "print(f\"Bubble Sort Time: {bubble_time:.4f} seconds\")\n",
    "print(f\"Merge Sort Time:  {merge_time:.4f} seconds\")\n",
    "print(f\"Merge sort is {bubble_time / merge_time:.2f} times faster than bubble sort\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's New?\n",
    "\n",
    "So all of that should be familiar from lab 8 (if not, go back through lab 8!), but we didn't spend long thinking about how to formally conceptualise and analyse why and how divide-and-conquer algorithms are more efficient than their brute force counterparts. As an important class of algorithm, there's significant work gone into thinking about how, why, and under what conditions Divide-and-Conque algorithms are efficient.\n",
    "\n",
    "First, let's *intuitively* understand why MergeSort specifically is $O(n \\log n)$. \n",
    "\n",
    "## Intuition behind MergeSort\n",
    "\n",
    "MergeSort is a recursive algorithm (it calls itself); the most intuitive way to think about it is to 'unroll' the recurrence and look at patterns in the first few levels:\n",
    "\n",
    "<div>\n",
    "   <img src=\"images/screenshot_rectree_mergesort.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "At a high level, some points that should be immediately obvious:\n",
    "- at the top leel, the amount of 'work' (i.e. items to sort) MergeSort has to do is a function of $n$ (the size of the input array),\n",
    "- at each level of recursion, the number of subproblems we're dealing with doubles,\n",
    "- at each level of recursion, the size of each subproblem halves,\n",
    "- the total amount of 'work' thus remains constant: at $n$.\n",
    "\n",
    "So that's one insight - at all levels the amount of 'work' to be done remains $n$. \n",
    "\n",
    "Now our problem is that if we were to do all that $n$ amount of work at the top level (without any dividing nonsense), we already know (from BubbleSort) that is $O(n^2)$. But if we Divide-and-Conquer, we can increase the efficiency of getting the same amount of work done. \n",
    "\n",
    "## Thinking through MergeSort in terms of base cases vs levels (an unrolled recurrence)\n",
    "\n",
    "First off, at all levels, the division operation is $c$ ($O(1)$) so we can effectively ignore it and focus on the merge and recurion elements! (as a recursive algorithm, in MergeSort they are jointly handled as part of recursive workflow).\n",
    "\n",
    "This takes us down to the base-case level (when as a single-valued array, they have been sorted), and in the combine step (where they are combined *in order*). We are therefore interested in how the number of base cases, and the number of combine ooerations scale wrt input size. For an $n$ lengthed array:\n",
    "- The **number of base case to combine** is also $n$. This gives us the $n$ of the $O(n \\log n)$.\n",
    "- The **number of recursive steps** is given by the *depth* of the recursion (i.e. how many steps does it have to take to recombine all the base cases into a single array again). As we are halving the number of subproblems at each step, this is given by $\\log n$ (the base 2 inverse exponential). This gives us the $\\log n$ of the $O(n \\log n)$.\n",
    "\n",
    "NB: We decomposed MergeSort in the above way because it is the most intuitive. However, it is isomorphic to (i.e., structurally equivalent to) the following decomposition, which we will use when deriving the recurrence relation:\n",
    "- **number of combine step**: $n$, as the number of comparisons we have to do between values is $n$ (every value has to be compared at one point during our combining steps)\n",
    "- **nummber of levels of recursion**: $\\log n$.\n",
    "\n",
    "#### More formally:\n",
    "\n",
    "1. **Top Level (Entire Array of Size $n$)**\n",
    "   - This is self-evident. \n",
    "   - We also make two recursive calls to sort the left and right halves.\n",
    "2. **Second Level (Two Subproblems of Size $n/2$)**\n",
    "   - Each of the two halves is of size $n/2$.\n",
    "   - Each half takes at most $O(n/2)$ time for merging.\n",
    "   - Since there are two such halves, the total time at this level is still $O(n)$.\n",
    "3. **Third Level (Four Subproblems of Size $n/4$)**\n",
    "   - Each subproblem is now $n/4$ in size.\n",
    "   - Each takes at most $O(n/4)$ time for merging.\n",
    "   - Since there are four such subproblems, the total time at this level remains $O(n)$.\n",
    "- **Generalizing to Deeper Levels**\n",
    "   - At each level of recursion, the total merging time remains $O(n)$ because there are always enough subproblems to sum up to $n$.\n",
    "   - The recursion continues until we reach base cases (single-element arrays).\n",
    "\n",
    "#### So at level $j$: \n",
    "1. Number of Subproblems at Level $j$\n",
    "   - At each level of recursion, the number of subproblems **doubles** compared to the previous level.\n",
    "   - We start with **1 subproblem** (the entire array), after $j$ levels, there are: $2^j \\text{ subproblems}$\n",
    "\n",
    "2. Size of Each Subproblem at Level $j$\n",
    "   - The array is divided in half at every level.\n",
    "   - After $j$ levels, each subproblem is of size: $\\frac{n}{2^j}$:\n",
    "      - after **one split**, the size is $ n/2 $, \n",
    "      - after **two splits**, it's $ n/4 $, \n",
    "      - and so on.\n",
    "\n",
    "3. Total Work Done at Level $j$\n",
    "   - As before, the division operation is done in constant time, so it drops out. This leaves us just with merging and recursion.\n",
    "   - Each of the $ 2^j $ subproblems requires at most $ O(n/2^j) $ time for merging.\n",
    "   - Since there are $ 2^j $ such subproblems, the total work done at level $j$ is: $2^j \\cdot \\frac{n}{2^j} = n$\n",
    "\n",
    "**The key point to get here is that the total work remains $ O(n) $ at **every** level!**\n",
    "\n",
    "4. Summing Over All Levels\n",
    "   - The recursion continues until we reach base cases where the subproblem size is **1**.\n",
    "   - The number of levels required to reduce $ n $ down to **1** is the number of times we can halve $ n $:\n",
    "   \n",
    "   $$\\log_2 n$$\n",
    "   \n",
    "   - Since we do **$ O(n) $ work at each level**, and there are $ O(\\log n) $ levels, the total time complexity is:\n",
    "   $$O(n \\log n)$$\n",
    "\n",
    "# Fomalising that intuition\n",
    "\n",
    "There are two conceptual tools that help us to make the jump from thinking about MergeSort speceifically, to generalising / abstracting to all Divide-and-conquer algorithms:\n",
    "1. A Recurrence Relation\n",
    "2. The Master Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Recurrence Relation\n",
    "\n",
    "*\"A recurrence relation is an equation that defines a sequence based on previous terms of the sequence. It expresses the value of a function (often representing the cost or size of a process) at size $n$, like $T(n)$, in terms of its values at smaller inputs, like  $T(n−1)$ or $T(n/2)$.\"*\n",
    "\n",
    "### Thinking through MergeSort in terms of Divide-&-Conquer recursion\n",
    "\n",
    "First, let's say $T(n)$ is the worst-case running time of the algorithm for an input of size $n$.\n",
    "\n",
    "For MergeSort: $T(n)$ can be decomposed into 3 separate processes at each step:\n",
    "   1. **Divide**: subdividing the problem into 2 pieces (of size $n/2$). \n",
    "      - *NB: the division part involves indexing an array, which is done in in constant-time $O(1)$, so it drops out*\n",
    "      - so we now have 2 problems.\n",
    "   2. **Conquer**: we then know that the algorithm will spend $T(n/2)$ on each sub-problem. \n",
    "      - so we now have 2 problems, each of size $T(n/2)$.\n",
    "   3. **Combine**: plus *some amount of time* combining the subproblems. \n",
    "      - the combine step involves comparing each $i$ and $j$ index: this scales linearly.\n",
    "      - so, that overhead will scale linearly with input size ->  $O(n)$\n",
    "      - so we now have 2 problems, each of size $T(n/2)$ and *some overhead that scales linearly*.\n",
    "\n",
    "\n",
    "\n",
    "The running time of MergeSort algorithm therefore satisfies the following recurrence relation:\n",
    "\n",
    "<div>\n",
    "   <img src=\"images/screenshot_mergesort.png\" width=\"300px\">\n",
    "</div>\n",
    "\n",
    "There are different ways of solving such a recurrence relation (i.e. make $T$ only appear on the left-hand side of the inequality). One would be to 'unroll' the recurrence as we did above, another is to use the Master Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Theorem\n",
    "2\n",
    "The Master Theorem provides a systematic way to analyze the runtime of divide-and-conquer algorithms by expressing their recurrence relation in a general form:\n",
    "\n",
    "$$\n",
    "T(n) = aT\\left(\\frac{n}{b}\\right) + n^c\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ a $ is the number of recursive subproblems created at each step.\n",
    "- $ b $ is the factor by which the problem size shrinks at each level.\n",
    "- $ c $ is the cost of the merging or combining step.\n",
    "\n",
    "This allows us to derive 3 important 'properties' of any Divide-and-conquery algorithm:\n",
    "- $1 + \\log_b n$ is the number of levels (the **depth**),\n",
    "  - $\\log_b$ is the number of recursive steps\n",
    "  - +1 is given by the top layer's existence\n",
    "- $\\frac{n}{b^j}$ is the **size** of subproblems at level $j$,\n",
    "- $a^j$ is the **number** of subproblems at level $j$.\n",
    "\n",
    "These 3 properties will be all we need to know of any divide-and-conquer algorithm to be able to analyse it using the Master Theorem.\n",
    "\n",
    "### Applying Master Theorem to MergeSort\n",
    "\n",
    "  <div>\n",
    "   <img src=\"images/mergesort_viz.png\" width=\"500px\" title=\"mergesort visualisation\">\n",
    "  </div>\n",
    "\n",
    "The running time of MergeSort satisfies the recurrence:\n",
    "\n",
    "\n",
    "$$T(n) = 2 \\, T\\left(\\frac{n}{2}\\right) + cn$$\n",
    "\n",
    "where:\n",
    "- $2$ subproblems are solved,\n",
    "- each subproblem has size $ \\frac{n}{2} $,\n",
    "- and $cn$ is the time to merge the two sorted halves.\n",
    "\n",
    "This gives the following properties for MergeSort:\n",
    "- $1 + \\log_2 n$ is the number of levels (the **depth**),e\n",
    "- $\\frac{n}{2^j}$ is the **size** of subproblems at level $j$,\n",
    "- $2^j$ is the **number** of subproblems at level $j$.\n",
    "\n",
    "We intuited all this previously!\n",
    "\n",
    "## Why We Care About the Master Theorem: General Form of Running Time\n",
    "\n",
    "At first this just complicated the more intuitive unrolling idea. But by formalising the Running Time in a more abstracted way, the Master Theorem lets us analyse generic Divide-and-Conquer problems in some interesting way.\n",
    "\n",
    "Following a pattern similar to the **recursion tree analysis**, we sum the work done **at each level** of recursion: given by the relationship between the 'properties' of any divide-and-conquer algorithm. Those properites were:\n",
    "  1. the depth of the recursion, \n",
    "  2. the scaling of the subproblems at each level (how much are they reduced), \n",
    "  2. and the number of subproblems produced at each level.\n",
    "\n",
    "Formally:\n",
    "- The recursion **depth** is $ \\log_b n $ (since the problem size reduces by a factor of $ b $ at each level).\n",
    "- At each level, the work done depends on:\n",
    "  1. **The number of subproblems** at that level. ($a^i$)\n",
    "  2. **The size of each subproblem** and the time required to process it. ($\\frac{n}{b^i}$)\n",
    "\n",
    "Putting it all together:\n",
    "These factors determine the **recurrence ratio** $ r $, which helps 1) classify the running time into one of three cases; 2) derives the total running time.\n",
    "\n",
    "$$\n",
    "r = \\text{recurrence ratio}\n",
    "$$\n",
    "\n",
    "## Classifying Time Complexity with the Master Theorem's Recurrence Ratio\n",
    "Formally, we can define the following:\n",
    "\n",
    "$$\n",
    "r = \\frac{a}{b^c}\n",
    "$$\n",
    "\n",
    "This makes intuitive sense if you consider:\n",
    "- $ a $ # of recursive subproblems created at each step.\n",
    "- $ b $ factor by which problem size shrinks at each level.\n",
    "- $ c $ cost of merging / combining.\n",
    "\n",
    "The total running time follows:\n",
    "\n",
    "$$\n",
    "T(n) = n^c \\sum_{i=0}^{\\log_b n} r^i\n",
    "$$\n",
    "\n",
    "which leads to the following cases:\n",
    "\n",
    "$$\n",
    "T(n) =\n",
    "\\begin{cases} \n",
    "O(n^c) & \\text{if } r < 1 \\quad (c > \\log_b a) \\quad \\text{(Root-dominated)} \\\\\n",
    "O(n^c \\log n) & \\text{if } r = 1 \\quad (c = \\log_b a) \\quad \\text{(Balanced)} \\\\\n",
    "O(n^{\\log_b a}) & \\text{if } r > 1 \\quad (c < \\log_b a) \\quad \\text{(Leaf-dominated)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Root-Dominated, Balanced, and Leaf-Dominated Cases\n",
    "The Master Theorem helps determine whether an algorithm's time complexity is:\n",
    "- **Root-dominated**: Work is mainly done at the top level (combining step).\n",
    "- **Balanced**: Work is evenly distributed across levels.\n",
    "- **Leaf-dominated**: Work is mainly done at the leaves (smallest subproblems).\n",
    "\n",
    "\n",
    "## ... getting back to MergeSort\n",
    "We know that for MergeSort:\n",
    "1. two sub-problems are created at evvery step\n",
    "  - $ a = 2$\n",
    "2. the problem is divided into **subproblems** of size $ n/2 $\n",
    "  - $ b = 2 $\n",
    "3. **combining step** takes linear time ($ c = 1 $)\n",
    "  - $ c = 1$ \n",
    "\n",
    "This gives us $r = 1$: it is a 'balanced\\linear' divide-and-conquer algorith (it produces two subproblems at each level, but each subproblem's size is halved). Thus its runtime is $O(n \\log n)$\n",
    "\n",
    "This makes intuitive sense. We already looked at why it is $O(n \\log n)$, and we also said in lab 8 that the 'magic' happens as MergeSort merges the lists into sorted lists or increasing size; this is done at the every combining stage of every recursive step. This validates what we already knew, but as a formalised theorem, the Master Theorem also gives us the tools to generalise this to the whole class of algorithms.\n",
    "\n",
    "## Extrapolating Out  \n",
    "As with MergeSort, consider an algorithm where:\n",
    "1. The **combining step** takes linear time ($ c = 1 $).\n",
    "2. The problem is divided into **subproblems** of size $ n/2 $ (i.e., $ b = 2 $).\n",
    "\n",
    "Thus, we compute:\n",
    "\n",
    "$$\n",
    "r = \\frac{a}{2}\n",
    "$$\n",
    "\n",
    "For different values of $ a $, we get different running times:\n",
    "\n",
    "| **Number of Subproblems ($ a $)** | **Running Time** | **Classification** |\n",
    "|--------------|----------------|------------------|\n",
    "| $ a = 1 $ | $ O(n) $ | **Root-dominated** (Cost dominated by the top level) |\n",
    "| $ a = 2 $ | $ O(n \\log n) $ | **Balanced** (Merge Sort case) |\n",
    "| $ a > 2 $ | $ O(n^{\\log_2 a}) $ | **Leaf-dominated** (Cost dominated by the leaves) |\n",
    "\n",
    "\n",
    "### Intuition Behind These Cases\n",
    "- **When $ a = 1 $:** \n",
    "  - Each recursive call reduces the problem size, but since there's only one subproblem at each step, the total work remains **linear**.\n",
    "  - The **top-level work (combining step)** dominates the complexity.\n",
    "  <div>\n",
    "   <img src=\"images/screenshot_rectree_a1.png\" width=\"200px\">\n",
    "  </div>  \n",
    "  \n",
    "- **When $ a = 2 $ (Merge Sort case):** \n",
    "  - Each level does roughly the same amount of work.\n",
    "  - The recursion **depth is $ \\log n $**, and each level takes $ O(n) $, so the total time is $ O(n \\log n) $.\n",
    "  - Work is **evenly distributed across levels**.\n",
    "  <div>\n",
    "   <img src=\"images/screenshot_rectree_mergesort.png\" width=\"500px\">\n",
    "  </div>\n",
    "\n",
    "- **When $ a > 2 $:**\n",
    "  - The number of subproblems grows **faster than the problem size shrinks**, leading to an increasing workload at the lower levels.\n",
    "  - The **leaves of the recursion tree** dominate the total complexity.\n",
    "  <div>\n",
    "   <img src=\"images/screenshot_rectree_a3.png\" width=\"500px\">\n",
    "  </div>\n",
    "\n",
    "(All visualisations of recursive trees are taken from the Kleinberg textbook, which also has great further explanations in it!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise on Master Theorem\n",
    "\n",
    "Use the Master Theorem to give time complexity of Sort-and-Count and say whether it is root-dominated, leaf-dominated or balanced.\n",
    "\n",
    "```\n",
    "Sort-and-Count(L)\n",
    "If the list has one element:\n",
    "    there are no inversions\n",
    "Else\n",
    "    Divide the list into two halves:\n",
    "        A contains the first ⌈n/2⌉ elements\n",
    "        B contains the remaining ⌊n/2⌋ elements\n",
    "    (rA, A) = Sort-and-Count(A)\n",
    "    (rB, B) = Sort-and-Count(B)\n",
    "    (r,L) = Merge-and-Count(A,B)\n",
    "Endif\n",
    "Return r =rA +rB +r, and the sorted list L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recurrence relation is $T(n) = 2 T(\\frac{n}{2}) + O(n)$. \n",
    "\n",
    "This is due to the fact that the `merge_and_count` algorithm you wrote in Exercise 5 of lab 8 has time complexity $O(n)$ and because we have two subproblems at each level, each of which is always half the size of the size at the previous level.\n",
    "\n",
    "We therefore have $a=2$, $b=2$, and $c=1$, so $r=\\frac{a}{b^c}= 1$, which - according to the Master Theorem - means that we have time complexity $O(n \\log n)$ and the algorithm is **balanced**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
